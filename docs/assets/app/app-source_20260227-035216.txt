FD_APP_SOURCE_V1
timestamp_utc: 20260227-035216
root: /

FILE: README_FD_AUTO.md
<<<
# FD Auto (Two-Flow)

This drop-in adds two GitHub Actions workflows:

1) Build App From Milestone Issue (Manual)
- Input: issue_number (milestone issue)
- Output: pushes a new app branch (default: app-<ms-id>-<UTC timestamp>)
- Artifacts: plan + Gemini raw outputs + assembled app zip

2) Tune App Branch (Manual)
- Input: branch (app branch name)
- Runs dry-run + unit tests.
- If red, iterates: send failing logs + selected files to Gemini, receive a patch bundle, apply, commit, push.
- Stops when green or max_attempts reached.
- Artifacts: logs + Gemini prompts/outputs per attempt.

Secrets required:
- FD_BOT_TOKEN (PAT or GitHub App token with contents:write, issues:read, pull requests optional)
- GEMINI_API_KEY
Optional secrets:
- GEMINI_MODEL (default: gemini-2.5-pro)
- GEMINI_ENDPOINT_BASE (default: https://generativelanguage.googleapis.com/v1beta)

Notes:
- Workflows remain on main.
- App branches do NOT need workflows; main workflow checks them out.

>>>

FILE: agent_guides/ROLE_PM.txt
<<<
ROLE: PM

Output FD_PATCH_V1 only.
Write ONLY these files:
- handoff/app_building_plan.md
- handoff/work_items/WI-001-code.md
- handoff/work_items/WI-002-docs.md
- handoff/work_items/WI-003-tests.md
- handoff/work_items/WI-004-publish.md

Do NOT write any files outside handoff/.


>>>

FILE: agent_guides/ROLE_BUILDER.txt
<<<
ROLE: BUILDER

Output FD_BUNDLE_V1 PART 1/Y only.
Then metadata lines:
work_item_id: WI-001
producer_role: BUILDER
artifact_type: repo_patch

Then FILE blocks with <<< and >>>.

No prose. Close every FILE block. No ellipses.


>>>

FILE: docs/index.md
<<<
---
layout: default
title: FD2
---

# FD2

This repository contains automation workflows and tooling.

>>>

FILE: src/__init__.py
<<<


>>>

FILE: src/fd_auto/patch_parse.py
<<<
from typing import List, Tuple
from dataclasses import dataclass

@dataclass
class FileEntry:
    path: str
    content: str

@dataclass
class Patch:
    kind: str  # patch | bundle
    work_item_id: str
    producer_role: str
    files: List[FileEntry]
    delete: List[str]

def _fail(msg: str) -> None:
    raise ValueError("FD_PARSE_FAIL: " + msg)

def parse_fd_patch_v1(text: str) -> Patch:
    t = (text or "").replace("\r\n","\n").replace("\r","\n").strip()
    if not t.startswith("FD_PATCH_V1"):
        _fail("missing FD_PATCH_V1 header")
    lines = t.split("\n")
    meta = {}
    files: List[FileEntry] = []
    delete: List[str] = []
    i = 1
    while i < len(lines):
        line = lines[i]
        if line.startswith("FILE:") or line.strip() in ("DELETE:", "END"):
            break
        if line.strip() == "":
            i += 1
            continue
        if ":" not in line:
            _fail("bad meta line: " + line[:120])
        k, v = line.split(":", 1)
        meta[k.strip()] = v.strip()
        i += 1

    def parse_file(j: int) -> Tuple[int, FileEntry]:
        header = lines[j]
        path = header[len("FILE:"):].strip()
        if path == "":
            _fail("empty FILE path")
        if j + 1 >= len(lines) or lines[j+1].strip() != "<<<":
            _fail("FILE missing <<< for path=" + path)
        k = j + 2
        buf = []
        while k < len(lines):
            if lines[k].strip() == ">>>":
                break
            buf.append(lines[k])
            k += 1
        if k >= len(lines):
            _fail("FILE missing >>> for path=" + path)
        return k + 1, FileEntry(path=path, content="\n".join(buf) + "\n")

    while i < len(lines):
        line = lines[i].strip()
        if line == "":
            i += 1
            continue
        if line.startswith("FILE:"):
            i, fe = parse_file(i)
            files.append(fe)
            continue
        if line == "DELETE:":
            i += 1
            while i < len(lines):
                l = lines[i].strip()
                if l == "":
                    i += 1
                    continue
                if l == "END":
                    break
                if l.startswith("-"):
                    delete.append(l[1:].strip())
                else:
                    _fail("bad DELETE line: " + l[:120])
                i += 1
            continue
        if line == "END":
            break
        _fail("unexpected line: " + line[:120])

    wi = meta.get("work_item_id","").strip()
    prod = meta.get("producer_role","").strip()
    if wi == "":
        _fail("missing work_item_id")
    if prod == "":
        _fail("missing producer_role")
    if not files:
        _fail("no FILE blocks")
    return Patch(kind="patch", work_item_id=wi, producer_role=prod, files=files, delete=delete)

def bundle_total_parts(raw: str) -> Tuple[int, int]:
    t = (raw or "").strip()
    if not t.startswith("FD_BUNDLE_V1"):
        return (1,1)
    first = t.splitlines()[0].strip()
    if "PART" not in first:
        return (1,1)
    toks = first.split()
    if len(toks) < 4:
        return (1,1)
    frac = toks[3]
    if "/" not in frac:
        return (1,1)
    a,b = frac.split("/",1)
    try:
        return (int(a), int(b))
    except Exception:
        return (1,1)

def _strip_bundle_header(raw: str) -> str:
    t = (raw or "").replace("\r\n","\n").replace("\r","\n").strip()
    if not t.startswith("FD_BUNDLE_V1"):
        _fail("bundle missing header")
    lines = t.split("\n")
    return "\n".join(lines[1:]).lstrip()

def parse_bundle_parts(parts: List[str]) -> Patch:
    if not parts:
        _fail("no parts")
    base: Patch | None = None
    seen = {}
    order: List[str] = []
    delete: List[str] = []
    for raw in parts:
        patch_text = "FD_PATCH_V1\n" + _strip_bundle_header(raw)
        p = parse_fd_patch_v1(patch_text)
        if base is None:
            base = p
        delete.extend(p.delete)
        for fe in p.files:
            if fe.path not in order:
                order.append(fe.path)
            seen[fe.path] = fe
    assert base is not None
    merged = [seen[p] for p in order]
    return Patch(kind="bundle", work_item_id=base.work_item_id, producer_role=base.producer_role, files=merged, delete=delete)

>>>

FILE: src/fd_auto/__init__.py
<<<


>>>

FILE: src/fd_auto/github_api.py
<<<
import json
import os
import urllib.request
from typing import Any, Dict, List

def _repo() -> str:
    r = (os.environ.get("GITHUB_REPOSITORY") or "").strip()
    if r == "":
        raise RuntimeError("FD_FAIL: missing GITHUB_REPOSITORY")
    return r

def _headers(token: str) -> Dict[str, str]:
    return {
        "Authorization": "Bearer " + token,
        "Accept": "application/vnd.github+json",
        "User-Agent": "fd-auto",
    }

def safe_get(d: Any, key: str, default: Any = "") -> Any:
    if isinstance(d, dict) and key in d:
        return d[key]
    return default

def get_issue(issue_number: int, token: str) -> Dict[str, Any]:
    repo = _repo()
    url = f"https://api.github.com/repos/{repo}/issues/{issue_number}"
    req = urllib.request.Request(url, headers=_headers(token), method="GET")
    with urllib.request.urlopen(req, timeout=60) as resp:
        return json.loads(resp.read().decode("utf-8"))

def create_comment(issue_number: int, body: str, token: str) -> None:
    repo = _repo()
    url = f"https://api.github.com/repos/{repo}/issues/{issue_number}/comments"
    payload = json.dumps({"body": body}).encode("utf-8")
    req = urllib.request.Request(url, headers=_headers(token), data=payload, method="POST")
    with urllib.request.urlopen(req, timeout=60):
        pass

def list_issues(token: str, state: str = "open") -> List[Dict[str, Any]]:
    repo = _repo()
    url = f"https://api.github.com/repos/{repo}/issues?state={state}&per_page=100"
    req = urllib.request.Request(url, headers=_headers(token), method="GET")
    with urllib.request.urlopen(req, timeout=60) as resp:
        return json.loads(resp.read().decode("utf-8"))

def list_comments(issue_number: int, token: str) -> List[Dict[str, Any]]:
    repo = _repo()
    url = f"https://api.github.com/repos/{repo}/issues/{issue_number}/comments?per_page=100"
    req = urllib.request.Request(url, headers=_headers(token), method="GET")
    with urllib.request.urlopen(req, timeout=60) as resp:
        return json.loads(resp.read().decode("utf-8"))

>>>

FILE: src/fd_auto/gemini_client.py
<<<
import json
import os
import time
import urllib.error
import urllib.request

DEFAULT_ENDPOINT = "https://generativelanguage.googleapis.com/v1beta"
DEFAULT_MODEL = "gemini-2.5-pro"

def _env_int(name: str, default: int) -> int:
    v = (os.environ.get(name) or "").strip()
    if v == "":
        return default
    try:
        return int(v)
    except Exception:
        return default

def _endpoint(base: str, model: str) -> str:
    b = (base or DEFAULT_ENDPOINT).rstrip("/")
    return b + "/models/" + model + ":generateContent"

def call_gemini(prompt: str, timeout_s: int = 900) -> str:
    api_key = (os.environ.get("GEMINI_API_KEY") or "").strip()
    if api_key == "":
        raise RuntimeError("FD_FAIL: missing GEMINI_API_KEY")
    model = (os.environ.get("GEMINI_MODEL") or DEFAULT_MODEL).strip()
    base = (os.environ.get("GEMINI_ENDPOINT_BASE") or DEFAULT_ENDPOINT).strip()
    url = _endpoint(base, model)

    retries = _env_int("FD_GEMINI_RETRIES", 2)
    think_budget = _env_int("FD_GEMINI_THINKING_BUDGET", 1024)
    max_out = _env_int("FD_GEMINI_MAX_OUTPUT_TOKENS", 0)  # 0 => omit
    resp_mime = (os.environ.get("FD_GEMINI_RESPONSE_MIME") or "text/plain").strip()

    def payload() -> dict:
        gen = {
            "temperature": 0.2,
            "responseMimeType": resp_mime,
            "thinkingConfig": {"includeThoughts": False, "thinkingBudget": (think_budget if think_budget > 0 else 1)},
        }
        if max_out > 0:
            gen["maxOutputTokens"] = max_out
        return {"contents": [{"role": "user", "parts": [{"text": prompt}]}], "generationConfig": gen}

    last_raw = ""
    for attempt in range(1, retries + 1):
        body = json.dumps(payload()).encode("utf-8")
        req = urllib.request.Request(url=url, data=body, method="POST")
        req.add_header("content-type", "application/json; charset=utf-8")
        req.add_header("x-goog-api-key", api_key)
        try:
            with urllib.request.urlopen(req, timeout=timeout_s) as resp:
                last_raw = resp.read().decode("utf-8", errors="replace")
        except urllib.error.HTTPError as e:
            b = ""
            try:
                b = e.read().decode("utf-8", errors="replace")
            except Exception:
                pass
            raise RuntimeError("FD_FAIL: gemini http=" + str(e.code) + " body=" + b[:800])
        except Exception:
            if attempt < retries:
                time.sleep(min(2 ** (attempt - 1), 4))
                continue
            raise

        data = json.loads(last_raw)
        cands = data.get("candidates") or []
        if isinstance(cands, list) and cands:
            c0 = cands[0]
            content = c0.get("content") if isinstance(c0, dict) else None
            parts = content.get("parts") if isinstance(content, dict) else None
            if isinstance(parts, list):
                texts = []
                for p in parts:
                    if isinstance(p, dict) and isinstance(p.get("text"), str):
                        texts.append(p["text"])
                if texts:
                    return "\n".join(texts)
        # allow retry if no parts
        if attempt < retries:
            continue
        raise RuntimeError("FD_FAIL: gemini parse raw=" + last_raw[:800])

>>>

FILE: src/fd_auto/util.py
<<<
import os
import re
from typing import Dict, Tuple

def env(name: str, default: str = "") -> str:
    v = os.environ.get(name, default)
    return (v or "").strip()

def require_env(name: str) -> str:
    v = env(name)
    if v == "":
        raise RuntimeError("FD_FAIL: missing env " + name)
    return v

def extract_field(text: str, key: str) -> str:
    for line in (text or "").splitlines():
        if line.startswith(key + ":"):
            return line.split(":", 1)[1].strip()
    return ""

def slugify(s: str) -> str:
    t = (s or "").lower().strip()
    t = re.sub(r"[^a-z0-9]+", "-", t)
    t = re.sub(r"-+", "-", t).strip("-")
    if t == "":
        return "app"
    return t[:40]

def task_key(s: str) -> Tuple[int, int, int, int, int, int, int, int]:
    t = (s or "").strip()
    if not re.match(r"^[0-9]+(\.[0-9]+)*$", t):
        return (9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999)
    parts = [int(x) for x in t.split(".")]
    pad = 8
    parts = parts[:pad] + [9999] * max(0, pad - len(parts))
    return tuple(parts)

def first_n_lines(s: str, n: int) -> str:
    return "\n".join((s or "").splitlines()[:n])

>>>

FILE: src/fd_auto/apply_patch.py
<<<
import shutil
from pathlib import Path
from src.fd_auto.patch_parse import Patch

def apply_patch(patch: Patch, repo_root: str) -> None:
    root = Path(repo_root)
    for rel in patch.delete:
        if rel.strip() == "":
            continue
        p = root / rel
        if p.is_dir():
            shutil.rmtree(p, ignore_errors=True)
        elif p.exists():
            try:
                p.unlink()
            except Exception:
                pass
    for fe in patch.files:
        path = root / fe.path
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(fe.content, encoding="utf-8")

>>>

FILE: tools/fd_auto_apply_snapshot.py
<<<
#!/usr/bin/env python3
import os
from pathlib import Path

TEXT_EXT = {".py",".md",".yml",".yaml",".json",".txt",".html",".css",".js",".ts",".csv"}

def _fail(msg: str) -> None:
    raise RuntimeError("FD_FAIL: " + msg)

def _is_text_path(rel: str) -> bool:
    return Path(rel).suffix.lower() in TEXT_EXT

def apply_snapshot(snapshot_text: str, repo_root: Path) -> None:
    t = (snapshot_text or "").replace("\r\n","\n").replace("\r","\n")
    if not t.strip().startswith("FD_APP_SOURCE_V1"):
        _fail("snapshot missing header")
    lines = t.split("\n")
    i = 0
    # skip header/meta until first FILE:
    while i < len(lines) and not lines[i].startswith("FILE:"):
        i += 1
    while i < len(lines):
        line = lines[i]
        if not line.startswith("FILE:"):
            i += 1
            continue
        rel = line.split(":",1)[1].strip()
        if rel == "":
            _fail("empty FILE path")
        if i+1 >= len(lines) or lines[i+1].strip() != "<<<":
            _fail("missing <<< for " + rel)
        j = i + 2
        buf = []
        while j < len(lines):
            if lines[j].strip() == ">>>":
                break
            buf.append(lines[j])
            j += 1
        if j >= len(lines):
            _fail("missing >>> for " + rel)
        i = j + 1
        if not _is_text_path(rel):
            continue
        out_path = repo_root / rel
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text("\n".join(buf) + "\n", encoding="utf-8")

def main() -> int:
    import sys
    if len(sys.argv) < 2:
        print("usage: fd_auto_apply_snapshot.py <snapshot_file>")
        return 2
    repo_root = Path(os.getcwd())
    snap = Path(sys.argv[1])
    if not snap.exists():
        _fail("snapshot file not found " + str(snap))
    txt = snap.read_text(encoding="utf-8", errors="ignore")
    apply_snapshot(txt, repo_root)
    print("FD_OK: applied snapshot")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

>>>

FILE: tools/fd_auto_tune_branch.py
<<<
#!/usr/bin/env python3
import datetime
import sys

import os
import subprocess
import tempfile
import glob
from pathlib import Path

sys.path.insert(0, os.path.abspath(os.getcwd()))

from src.fd_auto.gemini_client import call_gemini
from src.fd_auto.patch_parse import parse_bundle_parts, bundle_total_parts
from src.fd_auto.apply_patch import apply_patch
from src.fd_auto.util import first_n_lines

def _run(cmd, cwd):
    return subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)

def _write(p: Path, s: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(s, encoding="utf-8", errors="ignore")

def _call_bundle(prompt: str, out_dir: Path) -> list[str]:
    parts = []
    first = call_gemini(prompt, timeout_s=900)
    parts.append(first)
    _write(out_dir / "part_1.txt", first)
    x, y = bundle_total_parts(first)
    if y <= 1:
        return parts
    cur = x
    while cur < y and cur < 8:
        cur += 1
        cont = prompt + "\n\nCONTINUE\nReturn ONLY: FD_BUNDLE_V1 PART " + str(cur) + "/" + str(y) + "\nDo not repeat earlier parts.\n"
        nxt = call_gemini(cont, timeout_s=900)
        parts.append(nxt)
        _write(out_dir / ("part_" + str(cur) + ".txt"), nxt)
    _write(out_dir / "bundle_full.txt", "\n\n".join(parts))
    return parts

def _upload_snapshot_chunks(snapshot_text: str, workflow_name: str, out_dir: Path) -> None:
    # Multi-call upload so the model can see full repo context without exceeding request size limits.
    if snapshot_text.strip() == "":
        return
    max_chars = int(os.environ.get("FD_SNAPSHOT_MAX_CHARS","180000") or "180000")
    chunk_chars = int(os.environ.get("FD_SNAPSHOT_CHUNK_CHARS","50000") or "50000")
    txt = snapshot_text[:max_chars]
    total = (len(txt) + chunk_chars - 1) // chunk_chars
    if total < 1:
        total = 1
    for i in range(total):
        a = i * chunk_chars
        b = min(len(txt), (i + 1) * chunk_chars)
        chunk = txt[a:b]
        # Upload snapshot to the model in multiple calls (ACK-only) to provide full context.
        _upload_snapshot_chunks(snapshot_text, workflow_name, artifacts / ("snapshot_upload_attempt_" + str(attempt)))
        prompt = ""
        prompt += "ROLE: BUILDER\n"
        prompt += "TASK: Receive repository snapshot chunk. Do not propose fixes yet.\n"
        prompt += "WORKFLOW_NAME: " + workflow_name + "\n"
        prompt += "INSTRUCTION: Reply with exactly: ACK " + str(i+1) + "/" + str(total) + "\n"
        prompt += "SNAPSHOT_CHUNK " + str(i+1) + "/" + str(total) + "\n"
        prompt += chunk + "\n"
        (out_dir / ("snapshot_chunk_" + str(i+1) + "_prompt.txt")).write_text(prompt, encoding="utf-8", errors="ignore")
        resp = call_gemini(prompt, timeout_s=900)
        (out_dir / ("snapshot_chunk_" + str(i+1) + "_response.txt")).write_text(resp, encoding="utf-8", errors="ignore")

def main() -> int:
    import sys
    if len(sys.argv) < 4:
        print("usage: fd_auto_tune_branch.py <branch> <workflow_name> <max_attempts>")
        return 2
    branch = sys.argv[1].strip()
    workflow_name = sys.argv[2].strip() or "dry_run_and_unittest"
    max_attempts_arg = int((sys.argv[3].strip() or "3"))
    if branch == "":
        return 2

    repo_root = os.getcwd()
    max_attempts = max_attempts_arg

    artifacts = Path(tempfile.mkdtemp(prefix="fd_tune_artifacts_"))
    _write(artifacts / "branch.txt", branch + "\n")

    subprocess.check_call(["git","checkout",branch])

    for attempt in range(1, max_attempts + 1):
    
        # Install deps if requirements present
        if Path("requirements.txt").exists():
            _run([sys.executable,"-m","pip","install","-r","requirements.txt"], repo_root)

        dry = None
        tests = None
        if workflow_name == "dry_run_only":
            dry = _run(["python","src/main.py","--dry-run"], repo_root)
            _write(artifacts / ("dry_run_attempt_" + str(attempt) + ".log"), dry.stdout)
            ok = (dry.returncode == 0)
        elif workflow_name == "unittest_only":
            tests = _run(["python","-m","unittest","discover","-s","tests"], repo_root)
            _write(artifacts / ("tests_attempt_" + str(attempt) + ".log"), tests.stdout)
            ok = (tests.returncode == 0)
        else:
            dry = _run(["python","src/main.py","--dry-run"], repo_root)
            _write(artifacts / ("dry_run_attempt_" + str(attempt) + ".log"), dry.stdout)
            tests = _run(["python","-m","unittest","discover","-s","tests"], repo_root)
            _write(artifacts / ("tests_attempt_" + str(attempt) + ".log"), tests.stdout)
            ok = (dry.returncode == 0 and tests.returncode == 0)

        if ok:
            print("FD_OK: green")
            return 0

        # Ask Gemini for patch
        dry_rc = str(dry.returncode) if dry is not None else "NA"
        dry_out = first_n_lines(dry.stdout, 200) if dry is not None else ""
        test_rc = str(tests.returncode) if tests is not None else "NA"
        test_out = first_n_lines(tests.stdout, 200) if tests is not None else ""
        failing = "WORKFLOW_NAME=" + workflow_name + "\nDRY_RUN_RC=" + dry_rc + "\n" + dry_out + "\n\nTEST_RC=" + test_rc + "\n" + test_out
        snap_dir = os.path.join(repo_root, "docs", "assets", "app")
        snaps = sorted(glob.glob(os.path.join(snap_dir, "app-source_*.txt")))
        snapshot_path = snaps[-1] if snaps else ""
        snapshot_text = ""
        if snapshot_path:
            snapshot_text = open(snapshot_path, "r", encoding="utf-8", errors="ignore").read()
        max_chars = int(os.environ.get("FD_SNAPSHOT_MAX_CHARS","180000") or "180000")
        chunk_chars = int(os.environ.get("FD_SNAPSHOT_CHUNK_CHARS","50000") or "50000")
        snapshot_snip = snapshot_text[:max_chars]
        prompt = ""
        prompt += "ROLE: BUILDER\n"
        prompt += "TASK: Fix the application to make dry-run and unit tests pass.\n"
        prompt += "OUTPUT: FD_BUNDLE_V1 PART 1/Y only. No prose. Close every FILE block.\n"
        prompt += "You MUST include an updated full repository snapshot file at: docs/assets/app/app-source_<timestamp>.txt\n"
        prompt += "CONTEXT: failing logs follow.\n\n" + failing + "\n\n"

        _write(artifacts / ("fix_prompt_attempt_" + str(attempt) + ".txt"), prompt)
        parts = _call_bundle(prompt, artifacts / ("fix_bundle_attempt_" + str(attempt)))
        patch = parse_bundle_parts(parts)
        apply_patch(patch, repo_root)
        # If the model returned a new snapshot file, apply it by slicing into real files.
        snap_dir2 = os.path.join(repo_root, "docs", "assets", "app")
        snaps2 = sorted(glob.glob(os.path.join(snap_dir2, "app-source_*.txt")))
        if snaps2:
            newest = snaps2[-1]
            subprocess.check_call(["python3","tools/fd_auto_apply_snapshot.py", newest], cwd=repo_root)

        subprocess.check_call(["git","add","-A"])
        try:
            subprocess.check_call(["git","commit","-m","FD tune attempt " + str(attempt)])
        except Exception:
            pass
        subprocess.check_call(["git","push","--force-with-lease"])
    print("FD_FAIL: tuning attempts exhausted")
    return 1

if __name__ == "__main__":
    raise SystemExit(main())

>>>

FILE: tools/fd_auto_build_from_issue.py
<<<
#!/usr/bin/env python3
import datetime
import sys

import os
import shutil
import subprocess
import tempfile
from pathlib import Path

sys.path.insert(0, os.path.abspath(os.getcwd()))

from src.fd_auto.github_api import get_issue, create_comment
from src.fd_auto.util import require_env, extract_field, slugify
from src.fd_auto.gemini_client import call_gemini
from src.fd_auto.patch_parse import parse_fd_patch_v1, parse_bundle_parts, bundle_total_parts
from src.fd_auto.apply_patch import apply_patch

def _write(path: Path, s: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(s, encoding="utf-8", errors="ignore")

def _call_bundle(prompt: str, out_dir: Path) -> list[str]:
    parts = []
    first = call_gemini(prompt, timeout_s=900)
    parts.append(first)
    _write(out_dir / "part_1.txt", first)
    x, y = bundle_total_parts(first)
    if y <= 1:
        return parts
    cur = x
    # hard cap; user can tune later
    while cur < y and cur < 8:
        cur += 1
        cont = prompt + "\n\nCONTINUE\nReturn ONLY: FD_BUNDLE_V1 PART " + str(cur) + "/" + str(y) + "\nDo not repeat earlier parts.\n"
        nxt = call_gemini(cont, timeout_s=900)
        parts.append(nxt)
        _write(out_dir / ("part_" + str(cur) + ".txt"), nxt)
    _write(out_dir / "bundle_full.txt", "\n\n".join(parts))
    return parts

def main() -> int:
    import sys
    if len(sys.argv) < 2:
        print("usage: fd_auto_build_from_issue.py <milestone_issue_number>")
        return 2
    issue_number = int(sys.argv[1])

    token = require_env("FD_BOT_TOKEN")
    repo_root = os.getcwd()

    issue = get_issue(issue_number, token)
    body = (issue.get("body") or "")
    title = (issue.get("title") or "")
    ms_id = extract_field(body, "Milestone ID") or "MS-01"
    app_slug = slugify(title)
    ts = datetime.datetime.now(datetime.timezone.utc).strftime("%Y%m%d-%H%M%S")
    branch = "app-" + ms_id.lower() + "-" + ts

    artifacts = Path(tempfile.mkdtemp(prefix="fd_build_artifacts_"))
    _write(artifacts / "milestone_issue.txt", body)
    subprocess.check_call(["python3","tools/fd_auto_make_snapshot.py"], cwd=repo_root)

    # 1) Plan (PM): FD_PATCH_V1 handoff-only
    pm_prompt = open("agent_guides/ROLE_PM.txt","r",encoding="utf-8",errors="ignore").read() if os.path.exists("agent_guides/ROLE_PM.txt") else "ROLE: PM\nOutput FD_PATCH_V1 with handoff files only.\n"
    plan_prompt = pm_prompt + "\n\nMILESTONE_TITLE\n" + title + "\n\nMILESTONE_BODY\n" + body + "\n"
    _write(artifacts / "plan_prompt.txt", plan_prompt)

    plan_out = ""
    patch = None
    last_err = ""
    for attempt in range(1,4):
        plan_out = call_gemini(plan_prompt, timeout_s=900)
        _write(artifacts / ("plan_output_attempt_" + str(attempt) + ".txt"), plan_out)
        try:
            patch = parse_fd_patch_v1(plan_out)
            break
        except Exception as exc:
            last_err = str(exc)
            continue
    if patch is None:
        create_comment(issue_number, "FD_FAIL: plan parse failed\nERROR=" + last_err, token)
        return 1

    # Apply plan into repo (handoff)
    apply_patch(patch, repo_root)

    # 2) Code bundle
    plan_text = ""
    plan_path = Path(repo_root) / "handoff" / "app_building_plan.md"
    if plan_path.exists():
        plan_text = plan_path.read_text(encoding="utf-8", errors="ignore")
    code_prompt = open("agent_guides/ROLE_BUILDER.txt","r",encoding="utf-8",errors="ignore").read() if os.path.exists("agent_guides/ROLE_BUILDER.txt") else ""
    code_prompt += "\n\nTASK\nGenerate FULL APPLICATION CODE ONLY.\n"
    code_prompt += "\n\nAPP_BUILDING_PLAN\n" + plan_text + "\n"
    code_prompt += "\nRULES\n- Output FD_BUNDLE_V1 PART 1/Y\n- Close every FILE block with >>>\n"
    _write(artifacts / "code_prompt.txt", code_prompt)
    code_parts = _call_bundle(code_prompt, artifacts / "code_bundle")
    code_patch = parse_bundle_parts(code_parts)
    apply_patch(code_patch, repo_root)

    # 3) Docs bundle
    docs_prompt = open("agent_guides/ROLE_BUILDER.txt","r",encoding="utf-8",errors="ignore").read() if os.path.exists("agent_guides/ROLE_BUILDER.txt") else ""
    docs_prompt += "\n\nTASK\nGenerate COMPREHENSIVE DOCUMENTATION ONLY.\n"
    docs_prompt += "- Write README.md and docs/howto.md and docs/troubleshooting.md\n"
    docs_prompt += "\n\nAPP_BUILDING_PLAN\n" + plan_text + "\n"
    docs_prompt += "\nRULES\n- Output FD_BUNDLE_V1 PART 1/Y\n- Close every FILE block with >>>\n"
    _write(artifacts / "docs_prompt.txt", docs_prompt)
    docs_parts = _call_bundle(docs_prompt, artifacts / "docs_bundle")
    docs_patch = parse_bundle_parts(docs_parts)
    apply_patch(docs_patch, repo_root)

    # 4) Tests bundle
    tests_prompt = open("agent_guides/ROLE_BUILDER.txt","r",encoding="utf-8",errors="ignore").read() if os.path.exists("agent_guides/ROLE_BUILDER.txt") else ""
    tests_prompt += "\n\nTASK\nGenerate UNIT TESTS ONLY.\n"
    tests_prompt += "- Write tests/ files for src/ modules\n"
    tests_prompt += "- Ensure tests run with: python -m unittest discover -s tests\n"
    tests_prompt += "\n\nAPP_BUILDING_PLAN\n" + plan_text + "\n"
    tests_prompt += "\nRULES\n- Output FD_BUNDLE_V1 PART 1/Y\n- Close every FILE block with >>>\n"
    _write(artifacts / "tests_prompt.txt", tests_prompt)
    tests_parts = _call_bundle(tests_prompt, artifacts / "tests_bundle")
    tests_patch = parse_bundle_parts(tests_parts)
    apply_patch(tests_patch, repo_root)
    subprocess.check_call(["python3","tools/fd_auto_make_snapshot.py"], cwd=repo_root)

# 3) (Optional) docs and tests are deferred; this build flow only creates app branch from code bundle.
    # Users run Tune flow to add docs/tests using branch input and extra env keys.

    # Publish branch
    subprocess.check_call(["git","checkout","-B", branch])
    subprocess.check_call(["git","add","-A"])
    try:
        subprocess.check_call(["git","commit","-m","FD build " + branch])
    except Exception:
        pass
    subprocess.check_call(["git","push","-u","origin", branch, "--force-with-lease"])

    create_comment(issue_number, "FD_OK: built app branch\nBRANCH=" + branch + "\nARTIFACTS_DIR=" + str(artifacts), token)
    print("FD_OK: branch=" + branch)
    print("FD_ARTIFACTS_DIR=" + str(artifacts))
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

>>>

FILE: tools/fd_auto_make_snapshot.py
<<<
#!/usr/bin/env python3
import datetime
import os
from pathlib import Path

TEXT_EXT = {".py",".md",".yml",".yaml",".json",".txt",".html",".css",".js",".ts",".csv"}

EXCLUDE_DIRS = {".git","__pycache__",".pytest_cache","node_modules",".venv","venv","docs/_site",".github"}

def _is_text_file(path: Path) -> bool:
    return path.suffix.lower() in TEXT_EXT

def _rel(p: Path, root: Path) -> str:
    return str(p.relative_to(root)).replace("\\","/")

def _should_skip_dir(rel: str) -> bool:
    for x in EXCLUDE_DIRS:
        if rel == x or rel.startswith(x + "/"):
            return True
    return False

def main() -> int:
    repo_root = Path(os.getcwd())
    out_dir = repo_root / "docs" / "assets" / "app"
    out_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.datetime.now(datetime.timezone.utc).strftime("%Y%m%d-%H%M%S")
    out_path = out_dir / ("app-source_" + ts + ".txt")

    max_file_bytes = int(os.environ.get("FD_SNAPSHOT_MAX_FILE_BYTES","600000") or "600000")

    lines = []
    lines.append("FD_APP_SOURCE_V1")
    lines.append("timestamp_utc: " + ts)
    lines.append("root: /")
    lines.append("")

    for dp, dn, fn in os.walk(repo_root):
        rel_dir = _rel(Path(dp), repo_root)
        if rel_dir == ".":
            rel_dir = ""
        if _should_skip_dir(rel_dir):
            dn[:] = []
            continue
        dn[:] = [d for d in dn if not _should_skip_dir((rel_dir + "/" + d).strip("/"))]
        for f in fn:
            p = Path(dp) / f
            rel = _rel(p, repo_root)
            if rel.startswith("docs/assets/app/app-source_"):
                continue
            if not _is_text_file(p):
                continue
            try:
                if p.stat().st_size > max_file_bytes:
                    continue
            except Exception:
                continue
            try:
                content = p.read_text(encoding="utf-8", errors="ignore")
            except Exception:
                continue
            lines.append("FILE: " + rel)
            lines.append("<<<")
            lines.extend(content.replace("\r\n","\n").replace("\r","\n").split("\n"))
            if not content.endswith("\n"):
                lines.append("")
            lines.append(">>>")
            lines.append("")

    out_path.write_text("\n".join(lines), encoding="utf-8")
    print("FD_OK: wrote " + str(out_path))
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

>>>
